---
layout: post
title: "Self-Hosted-Ai-Stack"
date: 2025-04-28 03:00:00 -0500
categories: Proxmox AI Docker Ubuntu
tags: Proxmox AI Docker Ubuntu
image:
 path: /assets/img/headers/Proxmox-Ollama.png
---

## Hardware

- Machine Used for AI: Lenovo P360 + Nvidia RTX A4000

## Software
- Proxmox - Host for ubuntu vm we will use to host the stack
- Ubuntu - 24.04 with nvidia drivers. 
- Ollama - Run models locally
- OpenWebui - enables a chatgpt like interface
- SearXNG - enables the models to search the web

There will be 3 stages to this document. 

    1. Prepare the Proxmox Host
    2. Install & Configure Ubuntu
    3. Ollama & OpenWebui Setup


## 1. Proxmox Setup

> *Note: We will be using Proxmox Version 9 but this has also worked with previous versions as well.*
{: .prompt-info }

Common bios tweaks -

1. Enable VT-d
2. Enable SR-IOV
3. Enable IOMMU

### 1.1. Enable IOMMU in Grub

> *Note: YOU WILL NEED TO ENABLE IOMMU IN BIOS.*
{: .prompt-warning }



#### INTEL CPU
```bash
nano /etc/default/grub
```
edit the line that starts with *GRUB_CMDLINE_LINUX_DEFAULT*

```bash
GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt"
```
#### AMD CPU
```bash
GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt"
```

##### Update grub and reboot 
```bash
update-grub
reboot
```
##### Verify
```bash
dmesg | grep -e DMAR -e IOMMU
```
Should display output that says Iommu Enabled

POST SCREENSHOT 

### 1.2. Load VFIO Modules

create file `/etc/modules-load.d/vfio.conf`

```bash
nano /etc/modules-load.d/vfio.conf
```
```bash
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
```

### 1.3. Identify GPU PCI IDs
`lspci -nn | grep -i nvidia`

```shell
01:00.0 VGA compatible controller [0300]: NVIDIA RTX A4000 [10de:2237]
01:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef]
```
**Note the IDs (10de:2237 and 10de:1aef).**

### 1.4. Bind GPU to VFIO

Create `/etc/modprobe.d/vfio.conf`

```bash
nano /etc/modprobe.d/vfio.conf
```
In this new file we will need the ID's from the previous step 
**10de:2237 and 10de:1aef**

```yaml
options vfio-pci ids=10de:2237,10de:1aef
```
##### Rebuild initramfs and reboot
```bash
update-initramfs -u
reboot
```
##### Check binding 
```bash
lspci -nnk -d 10de:2237
```
##### This should display 
**Kernel driver in use: vfio-pci**




##### At this point we should be okay to move on to the VM inside Proxmox
---
## 2. Proxmox VM Setup
> *Note: This section will assume you have already downloaded an ISO of ubuntu and have uploaded it to the proxmox host.*
{: .prompt-info }

### 2.1. Create the VM

#### Create an ubuntu machine with the following options
- System -> Machine Type = Q35
- System -> SCSI Controller = VirtIO SCSI
- Disks -> Disk Size = 200GB or more
- Disks -> Cache = No Cache
- CPU -> Sockets = 1 (if you have more sockets add the amount you have)
- CPU -> Cores = max amount you can spare
- CPU -> TYPE = HOST
- Memory = max amount you can spare
- Network = configure to your needs

#### optional settings
- Select the VM -> Options -> Enable QEMU Guest agent (once inside the vm run below)
```bash 
sudo apt install quemu-guest-agent
reboot
```

#### Edit the VM Config File
We created the vm but we have not started it yet. Before we do we need to pass through the GPU, the most important part. 

Im not really a fan of fumbling around in the proxmox GUI so i typically edit the config file. Take note of the VM Number proxmox assigned to the vm. If this is your first vm it will be 100. 

Edit VM config at `/etc/pve/qemu-server/100.conf`
```yaml
machine: q35
cpu: host,hidden=1,flags=+pcid

hostpci0: 01:00.0,pcie=1,x-vga=1
hostpci1: 01:00.1
```

### 2.2 Start the VM

##### Install ubuntu as normal.

###### You may need to expand LVM in the future. Post about that Here.

### 2.3 Configure Ubuntu

##### Update package list:

```bash
sudo apt update && sudo apt upgrade -y
```
##### Install Nvidia Drivers
```bash
sudo apt install nvidia-driver-550
```
##### Install Nvidia Tool Kit 

Since nvidia does not have a toolkit in the 24.04 repo yet we have to set our distribution to 22.04. This is probably not needed in the future but today its the only way it works for me =] 

```bash
distribution="ubuntu22.04"
```
```bash
curl -s -L https://nvidia.github.io/libnvidia-container/${distribution}/libnvidia-container.list |   sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit.gpg] https://#g' |   sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```
```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |   sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit.gpg
sudo apt update
sudo apt install -y nvidia-container-toolkit
```

##### Install Docker 
```bash
#prerequisites
sudo apt install apt-transport-https ca-certificates curl software-properties-common

#Add gpg key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

#Add docker to apt sources
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt update

sudo apt install docker-ce docker-compose

#log off and on
sudo usermod -aG docker ${USER}
# OR
sudo usermod -aG docker username

#check if docker is running
sudo systemctl status docker
```

##### Configure docker for nvidia
```bash
sudo nvidia-ctk runtime configure --runtime=docker

sudo systemctl restart docker
```

##### Verify Docker GPU Access
```bash
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

## 3. Ollama & OpenWebUI Setup

#### 3.1. Structure
Start by getting the following structure created. 

Im using bind mounts for persistent storage. Personally think its easier to manage and backup.
```bash
/home/ollama/ollama-webui/
├── docker-compose.yml
├── .env
├── ollama-data/          # Ollama models and configuration
│   └── models/           # Downloaded AI models will be stored here
└── open-webui-data/      # Open WebUI database and user data
    └── (various app data files)
```

```bash
#create the folder structure we need
sudo mkdir -p /home/ollama/ollama-webui/{ollama-data,open-webui-data}

#set permissions
sudo chown -R $USER:$USER /home/ollama/ollama-webui

cd /home/ollama/ollama-webui
```
#### 3.2 Docker-Compose File

###### compose is the easiest way to manage these containers and spin everything up and down with the same config at the same time. 

```bash
#if not already 
cd /home/ollama/ollama-webui

nano docker-compose.yml
```

```yml
#Docker Compose for ai-stack
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    volumes:
      - /home/ollama/ollama-webui/ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - ollama-network

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - /home/ollama/ollama-webui/open-webui-data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-}
      - WEBUI_AUTH=True
      - ENABLE_SIGNUP=True
    depends_on:
      - ollama
    networks:
      - ollama-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  ollama-network:
    driver: bridge
```
###### The compose file contains a few variables that you should store in an external .env file 
- WEBUI_SECRET_KEY
- OLLAMA_MODELS 

In the same directory as the compose flie create a .env

```bash
nano .env
```
Inside we need both the keys and their values

```yaml
WEBUI_SECRET_KEY=<SOMERANDOMSTRING>
OLLAMA_MODELS=/home/ollama/ollama-webui/ollama-data/models
```

##### Set Permissions for the directories

```bash
# The containers might need specific permissions
chmod 755 /home/ollama/ollama-webui/ollama-data
chmod 755 /home/ollama/ollama-webui/open-webui-data
```
##### With the permissions set and the compose file created we can now start up the stack and see what if any errors we get. 

```bash
docker-compose up -d
```
##### We'll want to check the logs on the stack as it builds, so to do that we can use 
```bash
docker-compose logs -f
```

##### periodically you may want to check the how much storage the models are using, 

```bash
sudo du -sh /home/ollama/ollama-webui/*
```

#### You should now be able to access the OpenWebUI on port 3000
`localhost:3000`

OR

`http://192.168.1.X:3000`
